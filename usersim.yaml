# usersim.yaml — dogfood: usersim testing itself
#
# Run from the project root:
#   usersim run
#
# Instrumentation runs usersim against the bundled examples and edge cases.
# No infinite recursion — it tests the *examples*, not this config.
version: 1

instrumentation: "python3 dogfood/instrumentation.py"
perceptions: dogfood/perceptions.py

users:
  - dogfood/users/*.py

scenarios:
  - name: data_processor_example
    description: "Full pipeline on the data-processor example (3 scenarios, 3 personas)"
  - name: scaffold_and_validate
    description: "usersim init into a temp dir — verify all expected files are created"
  - name: bad_config
    description: "Intentionally broken configs — verify clean error messages and non-zero exit"
  - name: judge_standalone
    description: "usersim judge with synthetic perceptions — verify standalone subcommand"
  - name: report_generation
    description: "Generate HTML report from known results — verify valid self-contained HTML"
  - name: full_integration
    description: "All subsystems in one pass — ensures every persona constraint fires with a real value (no vacuous antecedents)"
  - name: violation_health
    description: "Run data-processor and introspect its results — measures whether usersim's own constraints are doing useful work (healthy churn, not vacuous)"
  - name: broken_example
    description: "Intentionally broken instrumentation script — verifies usersim detects and surfaces the failure rather than swallowing it silently"

output:
  results: dogfood/results.json
  report:  dogfood/report.html
