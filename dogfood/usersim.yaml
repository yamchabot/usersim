# usersim.yaml — dogfood: usersim testing itself
#
# Run from the project root:
#   usersim run --config dogfood/usersim.yaml
#
# Instrumentation runs usersim against the bundled examples and edge cases.
# No infinite recursion — it tests the *examples*, not this config.
version: 1

instrumentation: "python3 instrumentation.py"
perceptions: perceptions.py

users:
  - users/*.py

scenarios:
  - name: data_processor_example
    description: "Full pipeline on the data-processor example (3 scenarios, 3 personas)"
  - name: scaffold_and_validate
    description: "usersim init into a temp dir — verify all expected files are created"
  - name: bad_config
    description: "Intentionally broken configs — verify clean error messages and non-zero exit"
  - name: judge_standalone
    description: "usersim judge with synthetic perceptions — verify standalone subcommand"
  - name: report_generation
    description: "Generate HTML report from known results — verify valid self-contained HTML"
  - name: full_integration
    description: "All subsystems in one pass — ensures every persona constraint fires with a real value (no vacuous antecedents)"

output:
  results: dogfood/results.json
  report:  dogfood/report.html
